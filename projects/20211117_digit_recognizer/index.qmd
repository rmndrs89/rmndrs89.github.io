---
title: "Digit recognizer"
description: |
  Recognizing handwritten digits with a convolutional neural network.
date: 2021-11-17
categories: [Python, PyTorch]
code-fold: true
code-copy: true
---

::: {.callout-note}
## Abstract

The problem with handwritten digits.
:::

```{python}
#| label: fig-example-images
#| fig-cap: A batch of example images of handwritten digits.
import numpy as np
import matplotlib.pyplot as plt
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from torch.utils.data import DataLoader
import torchvision
from torchvision import datasets, transforms

# Define the data transforms
data_transforms = transforms.Compose([
    transforms.ToTensor(),
    transforms.Normalize(mean=(0.1307,), std=(0.3081,))
])

# Get the MNIST dataset
ROOT_PATH = "~/Datasets/PyTorch"
train_dataset = torchvision.datasets.MNIST(root=ROOT_PATH, train=True, transform=data_transforms, download=True)
test_dataset = torchvision.datasets.MNIST(root=ROOT_PATH, train=False, transform=data_transforms, download=True)

# Define the dataloaders
BATCH_SIZE = 64
train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)
test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)

# Visualize images
for example_images, example_labels in train_loader:
    break
num_cols = 8
num_rows = int(example_images.shape[0] // num_cols)
fig, axs = plt.subplots(num_rows, num_cols, figsize=(5, 5))
fig.subplots_adjust(wspace=0.05, hspace=0.05)
for example_idx in range(example_images.shape[0]):
    ax = axs[example_idx % num_cols, example_idx // num_cols]
    ax.imshow(example_images[example_idx].squeeze(), cmap="gray")
    ax.axis("off")
# plt.tight_layout()
plt.show()
```

## Build a model

We start off with building a baseline model that simply takes all individual pixels and passes them through a fully connected network.

```{python}
class BaselineModel(nn.Module):
    def __init__(self):
        super().__init__()

        self.flatten = nn.Flatten()
        self.linear1 = nn.Linear(in_features=28 * 28, out_features=16)
        self.linear2 = nn.Linear(in_features=16, out_features=10)

    def forward(self, x):
        x = self.flatten(x)
        x = F.relu(self.linear1(x))
        x = self.linear2(x)
        return x
```

## Train the model

In `PyTorch` we need to make sure that the model and data live on the same device. Obviously, if we can use a GPU to speed up the training, we would do this. Therefore, we first check for the device, and then move the model and data there.

```{python}
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# Create an instance of the model
baseline_model = BaselineModel().to(device)

# Define the loss function to use
loss_fn = nn.CrossEntropyLoss()

# Use a common optimizer
optimizer = optim.Adam(baseline_model.parameters(), lr=0.001)
```

We define a separate function for validation that we can call after each training epoch:

```{python}
def eval_step(
    model: nn.Module,
    dataloader: DataLoader,
    loss_fn: nn.Module,
    device: torch.device
):
    # Put the model in eval mode
    model.eval()

    # Initialize performance metrics
    running_loss = 0.0
    correct = 0
    total = 0

    with torch.no_grad():
        for features, targets in dataloader:
            # Put the data and target to the device
            features, targets = features.to(device), targets.to(device)

            # Make predictions -- forward propagation
            predictions = model(features)

            # Calculate the loss
            loss = loss_fn(predictions, targets)

            # Track progress
            running_loss += loss.item()
            _, predicted = torch.max(predictions, 1)
            total += targets.size(0)
            correct += (predicted == targets).sum().item()

    avg_loss = running_loss / len(dataloader)
    accuracy = 100. * correct / total
    return avg_loss, accuracy

def train_step(
    model: nn.Module,
    dataloader: DataLoader,
    loss_fn: nn.Module,
    optimizer: optim.Optimizer,
    device: torch.device
):
    # Put the model in training mode
    model.train()

    # Initialize performance metrics
    running_loss = 0.0
    correct = 0
    total = 0

    for batch_idx, (features, targets) in enumerate(dataloader):
        # Put the data and targets to the correct device
        features, targets = features.to(device), targets.to(device)

        # Reset the optimizer
        optimizer.zero_grad()

        # Make predictions
        predictions = model(features)

        # Calculate the loss
        loss = loss_fn(predictions, targets)

        # Calculate the adjustments
        loss.backward()
        
        # Update the model
        optimizer.step()

        # Track progress
        running_loss += loss.item()
        _, predicted = torch.max(predictions, 1)
        total += targets.size(0)
        correct += (predicted == targets).sum().item()
    
    average_loss = running_loss / len(dataloader)
    accuracy = 100. * correct / total
    return average_loss, accuracy
```

Now train the model for several epochs on the training data, and after each epoch get the validation loss and accuracy.

```{python}
NUM_EPOCHS = 10

history = {metric: [] for metric in ["train_loss", "train_acc", "val_loss", "val_acc"]}

for epoch in range(NUM_EPOCHS):
    train_loss, train_acc = train_step(model=baseline_model, dataloader=train_loader, loss_fn=loss_fn, optimizer=optimizer, device=device)
    val_loss, val_acc = eval_step(model=baseline_model, dataloader=test_loader, loss_fn=loss_fn, device=device)
    history["train_loss"].append(train_loss)
    history["train_acc"].append(train_acc)
    history["val_loss"].append(val_loss)
    history["val_acc"].append(val_acc)
print(f"Final validation accuracy: {history['val_acc'][-1]}")
```

We have to check whether the training has converged and that we are not overfitting.

```{python}
fig, axs = plt.subplots(1, 2, sharex=True)
axs[0].plot(np.arange(NUM_EPOCHS) + 1, history["train_loss"], label="training")
axs[0].plot(np.arange(NUM_EPOCHS) + 1, history["val_loss"], label="validation")
axs[1].plot(np.arange(NUM_EPOCHS) + 1, history["train_acc"], label="training")
axs[1].plot(np.arange(NUM_EPOCHS) + 1, history["val_acc"], label="validation")
axs[0].set_ylabel("Loss")
axs[0].legend(loc="upper right")
axs[1].set_ylabel("Accuracy")
axs[1].legend(loc="lower right")
for ax in axs:
    ax.set_xlabel("Epoch")
plt.tight_layout()
plt.show()
```