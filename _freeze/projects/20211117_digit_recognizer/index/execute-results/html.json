{
  "hash": "cc3dbe9f07e9a2dd5a899cf4d7c8aecd",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"Digit recognizer\"\ndescription: |\n  Recognizing handwritten digits with a convolutional neural network.\ndate: 2021-11-17\ncategories: [Python, PyTorch]\ncode-fold: true\ncode-copy: true\n---\n\n::: {.callout-note}\n## Abstract\n\nThe problem with handwritten digits.\n:::\n\n::: {#cell-fig-example-images .cell execution_count=1}\n``` {.python .cell-code}\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader\nimport torchvision\nfrom torchvision import datasets, transforms\n\n# Define the data transforms\ndata_transforms = transforms.Compose([\n    transforms.ToTensor(),\n    transforms.Normalize(mean=(0.1307,), std=(0.3081,))\n])\n\n# Get the MNIST dataset\nROOT_PATH = \"~/Datasets/PyTorch\"\ntrain_dataset = torchvision.datasets.MNIST(root=ROOT_PATH, train=True, transform=data_transforms, download=True)\ntest_dataset = torchvision.datasets.MNIST(root=ROOT_PATH, train=False, transform=data_transforms, download=True)\n\n# Define the dataloaders\nBATCH_SIZE = 64\ntrain_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\ntest_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n\n# Visualize images\nfor example_images, example_labels in train_loader:\n    break\nnum_cols = 8\nnum_rows = int(example_images.shape[0] // num_cols)\nfig, axs = plt.subplots(num_rows, num_cols, figsize=(5, 5))\nfig.subplots_adjust(wspace=0.05, hspace=0.05)\nfor example_idx in range(example_images.shape[0]):\n    ax = axs[example_idx % num_cols, example_idx // num_cols]\n    ax.imshow(example_images[example_idx].squeeze(), cmap=\"gray\")\n    ax.axis(\"off\")\n# plt.tight_layout()\nplt.show()\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n\r0.3%\r0.7%\r1.0%\r1.3%\r1.7%\r2.0%\r2.3%\r2.6%\r3.0%\r3.3%\r3.6%\r4.0%\r4.3%\r4.6%\r5.0%\r5.3%\r5.6%\r6.0%\r6.3%\r6.6%\r6.9%\r7.3%\r7.6%\r7.9%\r8.3%\r8.6%\r8.9%\r9.3%\r9.6%\r9.9%\r10.2%\r10.6%\r10.9%\r11.2%\r11.6%\r11.9%\r12.2%\r12.6%\r12.9%\r13.2%\r13.6%\r13.9%\r14.2%\r14.5%\r14.9%\r15.2%\r15.5%\r15.9%\r16.2%\r16.5%\r16.9%\r17.2%\r17.5%\r17.9%\r18.2%\r18.5%\r18.8%\r19.2%\r19.5%\r19.8%\r20.2%\r20.5%\r20.8%\r21.2%\r21.5%\r21.8%\r22.1%\r22.5%\r22.8%\r23.1%\r23.5%\r23.8%\r24.1%\r24.5%\r24.8%\r25.1%\r25.5%\r25.8%\r26.1%\r26.4%\r26.8%\r27.1%\r27.4%\r27.8%\r28.1%\r28.4%\r28.8%\r29.1%\r29.4%\r29.8%\r30.1%\r30.4%\r30.7%\r31.1%\r31.4%\r31.7%\r32.1%\r32.4%\r32.7%\r33.1%\r33.4%\r33.7%\r34.0%\r34.4%\r34.7%\r35.0%\r35.4%\r35.7%\r36.0%\r36.4%\r36.7%\r37.0%\r37.4%\r37.7%\r38.0%\r38.3%\r38.7%\r39.0%\r39.3%\r39.7%\r40.0%\r40.3%\r40.7%\r41.0%\r41.3%\r41.7%\r42.0%\r42.3%\r42.6%\r43.0%\r43.3%\r43.6%\r44.0%\r44.3%\r44.6%\r45.0%\r45.3%\r45.6%\r45.9%\r46.3%\r46.6%\r46.9%\r47.3%\r47.6%\r47.9%\r48.3%\r48.6%\r48.9%\r49.3%\r49.6%\r49.9%\r50.2%\r50.6%\r50.9%\r51.2%\r51.6%\r51.9%\r52.2%\r52.6%\r52.9%\r53.2%\r53.6%\r53.9%\r54.2%\r54.5%\r54.9%\r55.2%\r55.5%\r55.9%\r56.2%\r56.5%\r56.9%\r57.2%\r57.5%\r57.9%\r58.2%\r58.5%\r58.8%\r59.2%\r59.5%\r59.8%\r60.2%\r60.5%\r60.8%\r61.2%\r61.5%\r61.8%\r62.1%\r62.5%\r62.8%\r63.1%\r63.5%\r63.8%\r64.1%\r64.5%\r64.8%\r65.1%\r65.5%\r65.8%\r66.1%\r66.4%\r66.8%\r67.1%\r67.4%\r67.8%\r68.1%\r68.4%\r68.8%\r69.1%\r69.4%\r69.8%\r70.1%\r70.4%\r70.7%\r71.1%\r71.4%\r71.7%\r72.1%\r72.4%\r72.7%\r73.1%\r73.4%\r73.7%\r74.0%\r74.4%\r74.7%\r75.0%\r75.4%\r75.7%\r76.0%\r76.4%\r76.7%\r77.0%\r77.4%\r77.7%\r78.0%\r78.3%\r78.7%\r79.0%\r79.3%\r79.7%\r80.0%\r80.3%\r80.7%\r81.0%\r81.3%\r81.7%\r82.0%\r82.3%\r82.6%\r83.0%\r83.3%\r83.6%\r84.0%\r84.3%\r84.6%\r85.0%\r85.3%\r85.6%\r85.9%\r86.3%\r86.6%\r86.9%\r87.3%\r87.6%\r87.9%\r88.3%\r88.6%\r88.9%\r89.3%\r89.6%\r89.9%\r90.2%\r90.6%\r90.9%\r91.2%\r91.6%\r91.9%\r92.2%\r92.6%\r92.9%\r93.2%\r93.6%\r93.9%\r94.2%\r94.5%\r94.9%\r95.2%\r95.5%\r95.9%\r96.2%\r96.5%\r96.9%\r97.2%\r97.5%\r97.9%\r98.2%\r98.5%\r98.8%\r99.2%\r99.5%\r99.8%\r100.0%\n\r100.0%\n\r2.0%\r4.0%\r6.0%\r7.9%\r9.9%\r11.9%\r13.9%\r15.9%\r17.9%\r19.9%\r21.9%\r23.8%\r25.8%\r27.8%\r29.8%\r31.8%\r33.8%\r35.8%\r37.8%\r39.7%\r41.7%\r43.7%\r45.7%\r47.7%\r49.7%\r51.7%\r53.7%\r55.6%\r57.6%\r59.6%\r61.6%\r63.6%\r65.6%\r67.6%\r69.6%\r71.5%\r73.5%\r75.5%\r77.5%\r79.5%\r81.5%\r83.5%\r85.5%\r87.4%\r89.4%\r91.4%\r93.4%\r95.4%\r97.4%\r99.4%\r100.0%\n\r100.0%\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![A batch of example images of handwritten digits.](index_files/figure-html/fig-example-images-output-2.png){#fig-example-images}\n:::\n:::\n\n\n## Build a model\n\nWe start off with building a baseline model that simply takes all individual pixels and passes them through a fully connected network.\n\n::: {#fa84ca82 .cell execution_count=2}\n``` {.python .cell-code}\nclass BaselineModel(nn.Module):\n    def __init__(self):\n        super().__init__()\n\n        self.flatten = nn.Flatten()\n        self.linear1 = nn.Linear(in_features=28 * 28, out_features=16)\n        self.linear2 = nn.Linear(in_features=16, out_features=10)\n\n    def forward(self, x):\n        x = self.flatten(x)\n        x = F.relu(self.linear1(x))\n        x = self.linear2(x)\n        return x\n```\n:::\n\n\n## Train the model\n\nIn `PyTorch` we need to make sure that the model and data live on the same device. Obviously, if we can use a GPU to speed up the training, we would do this. Therefore, we first check for the device, and then move the model and data there.\n\n::: {#fca3c56d .cell execution_count=3}\n``` {.python .cell-code}\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# Create an instance of the model\nbaseline_model = BaselineModel().to(device)\n\n# Define the loss function to use\nloss_fn = nn.CrossEntropyLoss()\n\n# Use a common optimizer\noptimizer = optim.Adam(baseline_model.parameters(), lr=0.001)\n```\n:::\n\n\nWe define a separate function for validation that we can call after each training epoch:\n\n::: {#19f1bd78 .cell execution_count=4}\n``` {.python .cell-code}\ndef eval_step(\n    model: nn.Module,\n    dataloader: DataLoader,\n    loss_fn: nn.Module,\n    device: torch.device\n):\n    # Put the model in eval mode\n    model.eval()\n\n    # Initialize performance metrics\n    running_loss = 0.0\n    correct = 0\n    total = 0\n\n    with torch.no_grad():\n        for features, targets in dataloader:\n            # Put the data and target to the device\n            features, targets = features.to(device), targets.to(device)\n\n            # Make predictions -- forward propagation\n            predictions = model(features)\n\n            # Calculate the loss\n            loss = loss_fn(predictions, targets)\n\n            # Track progress\n            running_loss += loss.item()\n            _, predicted = torch.max(predictions, 1)\n            total += targets.size(0)\n            correct += (predicted == targets).sum().item()\n\n    avg_loss = running_loss / len(dataloader)\n    accuracy = 100. * correct / total\n    return avg_loss, accuracy\n\ndef train_step(\n    model: nn.Module,\n    dataloader: DataLoader,\n    loss_fn: nn.Module,\n    optimizer: optim.Optimizer,\n    device: torch.device\n):\n    # Put the model in training mode\n    model.train()\n\n    # Initialize performance metrics\n    running_loss = 0.0\n    correct = 0\n    total = 0\n\n    for batch_idx, (features, targets) in enumerate(dataloader):\n        # Put the data and targets to the correct device\n        features, targets = features.to(device), targets.to(device)\n\n        # Reset the optimizer\n        optimizer.zero_grad()\n\n        # Make predictions\n        predictions = model(features)\n\n        # Calculate the loss\n        loss = loss_fn(predictions, targets)\n\n        # Calculate the adjustments\n        loss.backward()\n        \n        # Update the model\n        optimizer.step()\n\n        # Track progress\n        running_loss += loss.item()\n        _, predicted = torch.max(predictions, 1)\n        total += targets.size(0)\n        correct += (predicted == targets).sum().item()\n    \n    average_loss = running_loss / len(dataloader)\n    accuracy = 100. * correct / total\n    return average_loss, accuracy\n```\n:::\n\n\nNow train the model for several epochs on the training data, and after each epoch get the validation loss and accuracy.\n\n::: {#65b9e0d1 .cell execution_count=5}\n``` {.python .cell-code}\nNUM_EPOCHS = 10\n\nhistory = {metric: [] for metric in [\"train_loss\", \"train_acc\", \"val_loss\", \"val_acc\"]}\n\nfor epoch in range(NUM_EPOCHS):\n    train_loss, train_acc = train_step(model=baseline_model, dataloader=train_loader, loss_fn=loss_fn, optimizer=optimizer, device=device)\n    val_loss, val_acc = eval_step(model=baseline_model, dataloader=test_loader, loss_fn=loss_fn, device=device)\n    history[\"train_loss\"].append(train_loss)\n    history[\"train_acc\"].append(train_acc)\n    history[\"val_loss\"].append(val_loss)\n    history[\"val_acc\"].append(val_acc)\nprint(f\"Final validation accuracy: {history['val_acc'][-1]}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nFinal validation accuracy: 95.37\n```\n:::\n:::\n\n\nWe have to check whether the training has converged and that we are not overfitting.\n\n::: {#116aa3f9 .cell execution_count=6}\n``` {.python .cell-code}\nfig, axs = plt.subplots(1, 2, sharex=True)\naxs[0].plot(np.arange(NUM_EPOCHS) + 1, history[\"train_loss\"], label=\"training\")\naxs[0].plot(np.arange(NUM_EPOCHS) + 1, history[\"val_loss\"], label=\"validation\")\naxs[1].plot(np.arange(NUM_EPOCHS) + 1, history[\"train_acc\"], label=\"training\")\naxs[1].plot(np.arange(NUM_EPOCHS) + 1, history[\"val_acc\"], label=\"validation\")\naxs[0].set_ylabel(\"Loss\")\naxs[0].legend(loc=\"upper right\")\naxs[1].set_ylabel(\"Accuracy\")\naxs[1].legend(loc=\"lower right\")\nfor ax in axs:\n    ax.set_xlabel(\"Epoch\")\nplt.tight_layout()\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-7-output-1.png){}\n:::\n:::\n\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [],
    "includes": {}
  }
}