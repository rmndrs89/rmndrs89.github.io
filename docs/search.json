[
  {
    "objectID": "posts/20251112_first_post/index.html",
    "href": "posts/20251112_first_post/index.html",
    "title": "My first post",
    "section": "",
    "text": "Here is my first post!"
  },
  {
    "objectID": "talks.html",
    "href": "talks.html",
    "title": "Talks",
    "section": "",
    "text": "The Curious Case of Thomas Bayes\n\n\n\n\n\nWhat is Bayesian analysis and why should you use it? \n\n\n\n\n\nJan 6, 2026\n\n\nRobbin Romijnders\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts.html",
    "href": "posts.html",
    "title": "Blog",
    "section": "",
    "text": "My first post\n\n\n\n\n\n\nPython\n\n\nTensorFlow\n\n\n\nMy first blog post. \n\n\n\n\n\nOct 12, 2025\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site"
  },
  {
    "objectID": "projects/20251117_digit_recognizer/index.html",
    "href": "projects/20251117_digit_recognizer/index.html",
    "title": "Digit recognizer",
    "section": "",
    "text": "Abstract\n\n\n\nThe problem with handwritten digits.\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader\nimport torchvision\nfrom torchvision import datasets, transforms\n\n# Define the data transforms\ndata_transforms = transforms.Compose([\n    transforms.ToTensor(),\n    transforms.Normalize(mean=(0.1307,), std=(0.3081,))\n])\n\n# Get the MNIST dataset\nROOT_PATH = \"~/Datasets/PyTorch\"\ntrain_dataset = torchvision.datasets.MNIST(root=ROOT_PATH, train=True, transform=data_transforms, download=True)\ntest_dataset = torchvision.datasets.MNIST(root=ROOT_PATH, train=False, transform=data_transforms, download=True)\n\n# Define the dataloaders\nBATCH_SIZE = 64\ntrain_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\ntest_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n\n# Visualize images\nfor example_images, example_labels in train_loader:\n    break\nnum_cols = 8\nnum_rows = int(example_images.shape[0] // num_cols)\nfig, axs = plt.subplots(num_rows, num_cols, figsize=(5, 5))\nfig.subplots_adjust(wspace=0.05, hspace=0.05)\nfor example_idx in range(example_images.shape[0]):\n    ax = axs[example_idx % num_cols, example_idx // num_cols]\n    ax.imshow(example_images[example_idx].squeeze(), cmap=\"gray\")\n    ax.axis(\"off\")\n# plt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nFigure 1: A batch of example images of handwritten digits."
  },
  {
    "objectID": "projects/20251117_digit_recognizer/index.html#build-a-model",
    "href": "projects/20251117_digit_recognizer/index.html#build-a-model",
    "title": "Digit recognizer",
    "section": "Build a model",
    "text": "Build a model\nWe start off with building a baseline model that simply takes all individual pixels and passes them through a fully connected network.\n\n\nCode\nclass BaselineModel(nn.Module):\n    def __init__(self):\n        super().__init__()\n\n        self.flatten = nn.Flatten()\n        self.linear1 = nn.Linear(in_features=28 * 28, out_features=16)\n        self.linear2 = nn.Linear(in_features=16, out_features=10)\n\n    def forward(self, x):\n        x = self.flatten(x)\n        x = F.relu(self.linear1(x))\n        x = self.linear2(x)\n        return x"
  },
  {
    "objectID": "projects/20251117_digit_recognizer/index.html#train-the-model",
    "href": "projects/20251117_digit_recognizer/index.html#train-the-model",
    "title": "Digit recognizer",
    "section": "Train the model",
    "text": "Train the model\nIn PyTorch we need to make sure that the model and data live on the same device. Obviously, if we can use a GPU to speed up the training, we would do this. Therefore, we first check for the device, and then move the model and data there.\n\n\nCode\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# Create an instance of the model\nbaseline_model = BaselineModel().to(device)\n\n# Define the loss function to use\nloss_fn = nn.CrossEntropyLoss()\n\n# Use a common optimizer\noptimizer = optim.Adam(baseline_model.parameters(), lr=0.001)\n\n\nWe define a separate function for validation that we can call after each training epoch:\n\n\nCode\ndef eval_step(\n    model: nn.Module,\n    dataloader: DataLoader,\n    loss_fn: nn.Module,\n    device: torch.device\n):\n    # Put the model in eval mode\n    model.eval()\n\n    # Initialize performance metrics\n    running_loss = 0.0\n    correct = 0\n    total = 0\n\n    with torch.no_grad():\n        for features, targets in dataloader:\n            # Put the data and target to the device\n            features, targets = features.to(device), targets.to(device)\n\n            # Make predictions -- forward propagation\n            predictions = model(features)\n\n            # Calculate the loss\n            loss = loss_fn(predictions, targets)\n\n            # Track progress\n            running_loss += loss.item()\n            _, predicted = torch.max(predictions, 1)\n            total += targets.size(0)\n            correct += (predicted == targets).sum().item()\n\n    avg_loss = running_loss / len(dataloader)\n    accuracy = 100. * correct / total\n    return avg_loss, accuracy\n\ndef train_step(\n    model: nn.Module,\n    dataloader: DataLoader,\n    loss_fn: nn.Module,\n    optimizer: optim.Optimizer,\n    device: torch.device\n):\n    # Put the model in training mode\n    model.train()\n\n    # Initialize performance metrics\n    running_loss = 0.0\n    correct = 0\n    total = 0\n\n    for batch_idx, (features, targets) in enumerate(dataloader):\n        # Put the data and targets to the correct device\n        features, targets = features.to(device), targets.to(device)\n\n        # Reset the optimizer\n        optimizer.zero_grad()\n\n        # Make predictions\n        predictions = model(features)\n\n        # Calculate the loss\n        loss = loss_fn(predictions, targets)\n\n        # Calculate the adjustments\n        loss.backward()\n        \n        # Update the model\n        optimizer.step()\n\n        # Track progress\n        running_loss += loss.item()\n        _, predicted = torch.max(predictions, 1)\n        total += targets.size(0)\n        correct += (predicted == targets).sum().item()\n    \n    average_loss = running_loss / len(dataloader)\n    accuracy = 100. * correct / total\n    return average_loss, accuracy\n\n\nNow train the model for several epochs on the training data, and after each epoch get the validation loss and accuracy.\n\n\nCode\nNUM_EPOCHS = 10\n\nhistory = {metric: [] for metric in [\"train_loss\", \"train_acc\", \"val_loss\", \"val_acc\"]}\n\nfor epoch in range(NUM_EPOCHS):\n    train_loss, train_acc = train_step(model=baseline_model, dataloader=train_loader, loss_fn=loss_fn, optimizer=optimizer, device=device)\n    val_loss, val_acc = eval_step(model=baseline_model, dataloader=test_loader, loss_fn=loss_fn, device=device)\n    history[\"train_loss\"].append(train_loss)\n    history[\"train_acc\"].append(train_acc)\n    history[\"val_loss\"].append(val_loss)\n    history[\"val_acc\"].append(val_acc)\nprint(f\"Final validation accuracy: {history['val_acc'][-1]}\")\n\n\nFinal validation accuracy: 95.48\n\n\nWe have to check whether the training has converged and that we are not overfitting.\n\n\nCode\nfig, axs = plt.subplots(1, 2, sharex=True)\naxs[0].plot(np.arange(NUM_EPOCHS) + 1, history[\"train_loss\"], label=\"training\")\naxs[0].plot(np.arange(NUM_EPOCHS) + 1, history[\"val_loss\"], label=\"validation\")\naxs[1].plot(np.arange(NUM_EPOCHS) + 1, history[\"train_acc\"], label=\"training\")\naxs[1].plot(np.arange(NUM_EPOCHS) + 1, history[\"val_acc\"], label=\"validation\")\naxs[0].set_ylabel(\"Loss\")\naxs[0].legend(loc=\"upper right\")\naxs[1].set_ylabel(\"Accuracy\")\naxs[1].legend(loc=\"lower right\")\nfor ax in axs:\n    ax.set_xlabel(\"Epoch\")\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "talks/20260106_first_talk/index.html#a-note-of-history",
    "href": "talks/20260106_first_talk/index.html#a-note-of-history",
    "title": "The Curious Case of Thomas Bayes",
    "section": "A note of history",
    "text": "A note of history\n“Traditional” statistics (Dienes 2011)"
  },
  {
    "objectID": "talks/20260106_first_talk/index.html#bayesian-statistics",
    "href": "talks/20260106_first_talk/index.html#bayesian-statistics",
    "title": "The Curious Case of Thomas Bayes",
    "section": "Bayesian statistics",
    "text": "Bayesian statistics\n\n\n\n\n\n\nPlausibility\n\n\nBayesian statistics starts from the premise that we can assign degrees of plausibility to theories, and what we want our data to do is to tell us how to adjust these plausibilities.\n– Dienes (2011)\n\n\n\n\\[\n\\text{Pr}(\\text{hypothesis} \\mid \\text{data})\n\\]"
  },
  {
    "objectID": "talks/20260106_first_talk/index.html#getting-up",
    "href": "talks/20260106_first_talk/index.html#getting-up",
    "title": "The Curious Case of Thomas Bayes",
    "section": "Getting up",
    "text": "Getting up\n\nTurn off alarm\nGet out of bed"
  },
  {
    "objectID": "talks/20260106_first_talk/index.html#going-to-sleep",
    "href": "talks/20260106_first_talk/index.html#going-to-sleep",
    "title": "The Curious Case of Thomas Bayes",
    "section": "Going to sleep",
    "text": "Going to sleep\n\nGet in bed\nCount sheep"
  },
  {
    "objectID": "talks/20260106_first_talk/index.html#code-blocks",
    "href": "talks/20260106_first_talk/index.html#code-blocks",
    "title": "The Curious Case of Thomas Bayes",
    "section": "Code blocks",
    "text": "Code blocks\n\n\nFigure 1: Distributions of IQ for the two groups."
  },
  {
    "objectID": "talks/20260106_first_talk/index.html#the-illusion-of-objectivity",
    "href": "talks/20260106_first_talk/index.html#the-illusion-of-objectivity",
    "title": "The Curious Case of Thomas Bayes",
    "section": "The Illusion of Objectivity",
    "text": "The Illusion of Objectivity\n\n\nGoal: determine the effectiveness of vitamin C in treating the common cold (Berger and Berry. 1988)\nHypothesis: vitamin C has no effect on the common cold (“null hypothesis”)\nExperiment: 17 matched pairs\n\nC: subject receives vitamin C\nP: subject receives placebo\n\nOutcome: does the subjecting receiving C or the subject receiving P exhibit greater relief after treatment?\nResults:\n\n13 pairs: C is better\n4 pairs: P is better"
  },
  {
    "objectID": "talks/20260106_first_talk/index.html#explore-the-results-space",
    "href": "talks/20260106_first_talk/index.html#explore-the-results-space",
    "title": "The Curious Case of Thomas Bayes",
    "section": "Explore the results space",
    "text": "Explore the results space"
  },
  {
    "objectID": "talks/20260106_first_talk/index.html#discussion",
    "href": "talks/20260106_first_talk/index.html#discussion",
    "title": "The Curious Case of Thomas Bayes",
    "section": "Discussion",
    "text": "Discussion\n\n\nObserving 13 preferences for C is somewhat unexpected when H is true\nProof by contradiction\n\nAssume that H is true, and find a consequence R that logically follows from H yet is known to be false\n\nThis contradiction shows that H cannot be true\n\nIn standard statistics:\n\nH is the null hypothesis\nR is the observed or more extreme values\n\n\n\\(p\\) = 0.049"
  },
  {
    "objectID": "talks/20260106_first_talk/index.html#references",
    "href": "talks/20260106_first_talk/index.html#references",
    "title": "The Curious Case of Thomas Bayes",
    "section": "References",
    "text": "References\n\n\nBerger, James O., and Donald A. Berry. 1988. “Statistical Analysis and the Illusion of Objectivity.” American Scientist 76 (2): 159–65.\n\n\nDienes, Zoltan. 2011. “Bayesian Versus Orthodox Statistics: Which Side Are You On?” Perspectives on Psychological Science 6 (3): 274–90. https://doi.org/10.1177/1745691611406920."
  },
  {
    "objectID": "projects.html",
    "href": "projects.html",
    "title": "Projects",
    "section": "",
    "text": "Digit recognizer\n\n\n\nPython\n\n\nPyTorch\n\n\n\nRecognizing handwritten digits with a convolutional neural network.\n\n\n\nRobbin Romijnders\n\n\nNov 17, 2025\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Robbin Romijnders",
    "section": "",
    "text": "Tip\n\n\n\nI am currently rebuilding my personal website using Quarto, so you won’t see that much here yet.\n\n\nI am a biomedical engineer with expertise in digital signal processing and machine learning.\nMy core experience involves the analysis of complex, multimodal longitudinal datasets to understand dynamic physiological and health processes.\nA crucial part of my work is communicating technical findings effectively: I am adept at translating sophisticated results for a diverse audience, ensuring clarity and relevance whether I’m speaking with a fellow engineer or a practicing clinician."
  },
  {
    "objectID": "index.html#welcome",
    "href": "index.html#welcome",
    "title": "Robbin Romijnders",
    "section": "",
    "text": "Tip\n\n\n\nI am currently rebuilding my personal website using Quarto, so you won’t see that much here yet.\n\n\nI am a biomedical engineer with expertise in digital signal processing and machine learning.\nMy core experience involves the analysis of complex, multimodal longitudinal datasets to understand dynamic physiological and health processes.\nA crucial part of my work is communicating technical findings effectively: I am adept at translating sophisticated results for a diverse audience, ensuring clarity and relevance whether I’m speaking with a fellow engineer or a practicing clinician."
  }
]